{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Copernicus Marine Data Testing Scripts This repository contains automated testing scripts designed to monitor and verify the quality and availability of data from the Copernicus Data Store. Purpose The primary focus of these tests is not to validate the functionalities of the Copernicus Marine Toolbox commands themselves, but rather to: Ensure the integrity of the datasets Monitor the download speed Verify the availability of the latest data Current Tests The initial suite of tests includes: Regular checks for the presence of the most recent datasets Comparison between current and historical datasets to detect unintended modifications Measurement and logging of data download speeds Feedback and Contributions Your questions, feedback, and suggestions are highly welcome! Please reach out to: Guillaume Koenig Email: gkoenig@mercator-ocean.fr Feel free to use, modify, and share these scripts under an open, collaborative spirit.","title":"Home"},{"location":"#copernicus-marine-data-testing-scripts","text":"This repository contains automated testing scripts designed to monitor and verify the quality and availability of data from the Copernicus Data Store.","title":"Copernicus Marine Data Testing Scripts"},{"location":"#purpose","text":"The primary focus of these tests is not to validate the functionalities of the Copernicus Marine Toolbox commands themselves, but rather to: Ensure the integrity of the datasets Monitor the download speed Verify the availability of the latest data","title":"Purpose"},{"location":"#current-tests","text":"The initial suite of tests includes: Regular checks for the presence of the most recent datasets Comparison between current and historical datasets to detect unintended modifications Measurement and logging of data download speeds","title":"Current Tests"},{"location":"#feedback-and-contributions","text":"Your questions, feedback, and suggestions are highly welcome! Please reach out to: Guillaume Koenig Email: gkoenig@mercator-ocean.fr Feel free to use, modify, and share these scripts under an open, collaborative spirit.","title":"Feedback and Contributions"},{"location":"database_schema/","text":"Database Schema and Usage Currently, the database contains two schema: A simple one with a single table with rows describing each download attempt, including errors. This one is rigid and can only contain three errors. A more complex one with three tables: One containing the main information to reproduce a serie of tests ( OS used, version of the toolbox, version of the script, dates of execution), one containing all the datasets that have been tested, the service, version and variables that have been tested and if they were downloadable during the runs and one containing the errors associated with the datasets. The tables are linked between them via uuid (https://developer.mozilla.org/en-US/docs/Glossary/UUID). The collection of tests has a uuid, which is used as a foreign key in the datasets table to connect it with the first table, and each row of the dataset table has another uuid, which are used as foreign keys in the error table. To connect: Both the schema are on the same database. You will need information given in the readme of the database present in the project \"project-test-datasets-subsetting-toolbox\". The following command will ask a password that will also be given in the readme of the database. psql -h HOST -p PORT -U USERNAME -d DATABASE_NAME List tables: There should be four tables: - \"List_of_datasets\" ( the '\"' are necessary to access this table). This is the table of the simple schema. - testing_metadata containing the information about the test conditions. - datasets_tested containing the informations about the datasets tested. - errors containg the different errors. Tables columns: List_of_datasets : -id: UUID associated with each dataset ( uuid ) -dataset_id: Dataset identifier as can be used in the copernicusmarine toolbox ( String ) -dataset_version: Version of the dataset ( String ) -version_part: Part of the version tested ( String ) -service_name: Name of the service tested ( String ) -variable_name: Variable tested ( String ) -has_time_coordinate: Presence of time coordinate for the variable ( Boolean ) -last_available_time: Last available time for which there is data from the describe command ( String ) -region: Region of the dataset ( String ) -downloadable: If the dataset was downloadable ( Boolean ) -last_downloadable_time: Last available time for which there is from the subset command ( String ) -first_command: Command launched for the first try ( String ) -first_error: (Optional) Error if the first try did not work ( String ) -second_command: (Optional) Command launched for the second try ( String ) -second_error: (Optional) Error if the second try did not work ( String ) -third_command: (Optional) Command launched for the third try ( String ) -third_error: (Optional) Error if the third try did not work ( String ) testing_metadata -id: UUID associated which each run of the script ( uuid ) -start_time: Time of the start of the script's run ( DateTime ) -end_time: Time of the end of the script's run ( DateTime ) -linux_version: Version of the OS used for the run ( String ) -toolbox_version: Version of the toolbox used for the run ( String ) -script_version: Version of the script used for the run ( String ) datasets_tested -id: UUID associated with each dataset tested, not the same as the id of testing_metadata ( uuid ) -test_id: Foreign key connecting to the testing_metadata table, same as the id of testing_metadata for the run ( uuid ) -dataset_id: Identifier of the dataset as used with the copernicusmarine toolbox ( String ) -dataset_version: Version of the dataset ( String ) -version_part: Part of the version tested ( String ) -service_name: Name of the service tested ( String ) -variable_name: Variable tested ( String ) -command: Command launched for the first try ( Text ) -last_downloadable_time: Last available time for which there is from the subset command ( DateTime ) -downloadable: If the dataset was downloadable ( Boolean ) errors -id: UUID associated with each error, not the same as the id of datasets_tested or testing_metadata ( uuid ) -dataset_test_id: Foreign key connecting to the datasets_tested table, same as the id of datasets_tested for the dataset ( uuid ) -command: Command associated for the error ( Text ) -error_message: Message associated with the error ( Text ) The relationships between the tables is summarized in the following schema: Consulting tables Simple consultation You can see the list of tables, once connected, with the following command: \\dt You can look a bit better at the tables with the following commands. To see the first ten rows for example: SELECT * FROM my_table LIMIT 10; Or the last ten rows: SELECT * FROM my_table ORDER BY id DESC LIMIT 10; Consultation of complex schema If you want to consult the errors associated with a given run, you can run the following command, with replaced by the id found the in the table testing_metadata : SELECT tm.id AS run_id, tm.start_time, tm.end_time, dt.dataset_id, dt.dataset_version, dt.version_part, dt.service_name, dt.variable_name, e.id AS error_id, e.command AS error_command, e.error_message FROM testing_metadata tm JOIN datasets_tested dt ON tm.id = dt.test_id JOIN errors e ON dt.id = e.dataset_test_id WHERE tm.id = <RUN_ID> ORDER BY dt.dataset_id, e.id;","title":"Database"},{"location":"database_schema/#database-schema-and-usage","text":"Currently, the database contains two schema: A simple one with a single table with rows describing each download attempt, including errors. This one is rigid and can only contain three errors. A more complex one with three tables: One containing the main information to reproduce a serie of tests ( OS used, version of the toolbox, version of the script, dates of execution), one containing all the datasets that have been tested, the service, version and variables that have been tested and if they were downloadable during the runs and one containing the errors associated with the datasets. The tables are linked between them via uuid (https://developer.mozilla.org/en-US/docs/Glossary/UUID). The collection of tests has a uuid, which is used as a foreign key in the datasets table to connect it with the first table, and each row of the dataset table has another uuid, which are used as foreign keys in the error table.","title":"Database Schema and Usage"},{"location":"database_schema/#to-connect","text":"Both the schema are on the same database. You will need information given in the readme of the database present in the project \"project-test-datasets-subsetting-toolbox\". The following command will ask a password that will also be given in the readme of the database. psql -h HOST -p PORT -U USERNAME -d DATABASE_NAME","title":"To connect:"},{"location":"database_schema/#list-tables","text":"There should be four tables: - \"List_of_datasets\" ( the '\"' are necessary to access this table). This is the table of the simple schema. - testing_metadata containing the information about the test conditions. - datasets_tested containing the informations about the datasets tested. - errors containg the different errors.","title":"List tables:"},{"location":"database_schema/#tables-columns","text":"List_of_datasets : -id: UUID associated with each dataset ( uuid ) -dataset_id: Dataset identifier as can be used in the copernicusmarine toolbox ( String ) -dataset_version: Version of the dataset ( String ) -version_part: Part of the version tested ( String ) -service_name: Name of the service tested ( String ) -variable_name: Variable tested ( String ) -has_time_coordinate: Presence of time coordinate for the variable ( Boolean ) -last_available_time: Last available time for which there is data from the describe command ( String ) -region: Region of the dataset ( String ) -downloadable: If the dataset was downloadable ( Boolean ) -last_downloadable_time: Last available time for which there is from the subset command ( String ) -first_command: Command launched for the first try ( String ) -first_error: (Optional) Error if the first try did not work ( String ) -second_command: (Optional) Command launched for the second try ( String ) -second_error: (Optional) Error if the second try did not work ( String ) -third_command: (Optional) Command launched for the third try ( String ) -third_error: (Optional) Error if the third try did not work ( String ) testing_metadata -id: UUID associated which each run of the script ( uuid ) -start_time: Time of the start of the script's run ( DateTime ) -end_time: Time of the end of the script's run ( DateTime ) -linux_version: Version of the OS used for the run ( String ) -toolbox_version: Version of the toolbox used for the run ( String ) -script_version: Version of the script used for the run ( String ) datasets_tested -id: UUID associated with each dataset tested, not the same as the id of testing_metadata ( uuid ) -test_id: Foreign key connecting to the testing_metadata table, same as the id of testing_metadata for the run ( uuid ) -dataset_id: Identifier of the dataset as used with the copernicusmarine toolbox ( String ) -dataset_version: Version of the dataset ( String ) -version_part: Part of the version tested ( String ) -service_name: Name of the service tested ( String ) -variable_name: Variable tested ( String ) -command: Command launched for the first try ( Text ) -last_downloadable_time: Last available time for which there is from the subset command ( DateTime ) -downloadable: If the dataset was downloadable ( Boolean ) errors -id: UUID associated with each error, not the same as the id of datasets_tested or testing_metadata ( uuid ) -dataset_test_id: Foreign key connecting to the datasets_tested table, same as the id of datasets_tested for the dataset ( uuid ) -command: Command associated for the error ( Text ) -error_message: Message associated with the error ( Text ) The relationships between the tables is summarized in the following schema:","title":"Tables columns:"},{"location":"database_schema/#consulting-tables","text":"","title":"Consulting tables"},{"location":"database_schema/#simple-consultation","text":"You can see the list of tables, once connected, with the following command: \\dt You can look a bit better at the tables with the following commands. To see the first ten rows for example: SELECT * FROM my_table LIMIT 10; Or the last ten rows: SELECT * FROM my_table ORDER BY id DESC LIMIT 10;","title":"Simple consultation"},{"location":"database_schema/#consultation-of-complex-schema","text":"If you want to consult the errors associated with a given run, you can run the following command, with replaced by the id found the in the table testing_metadata : SELECT tm.id AS run_id, tm.start_time, tm.end_time, dt.dataset_id, dt.dataset_version, dt.version_part, dt.service_name, dt.variable_name, e.id AS error_id, e.command AS error_command, e.error_message FROM testing_metadata tm JOIN datasets_tested dt ON tm.id = dt.test_id JOIN errors e ON dt.id = e.dataset_test_id WHERE tm.id = <RUN_ID> ORDER BY dt.dataset_id, e.id;","title":"Consultation of complex schema"},{"location":"further_development/","text":"Further Development Planned Features Automated delivery of test results via email notifications Long-term comparison of dataset quality and availability Vision The long-term idea is to provide a reliable, automated monitoring pipeline for the Copernicus Data Store. This would allow users to: Track data availability trends Identify recurring download failures Ensure data integrity across updates Share transparent monitoring results with the community","title":"Further"},{"location":"further_development/#further-development","text":"","title":"Further Development"},{"location":"further_development/#planned-features","text":"Automated delivery of test results via email notifications Long-term comparison of dataset quality and availability","title":"Planned Features"},{"location":"further_development/#vision","text":"The long-term idea is to provide a reliable, automated monitoring pipeline for the Copernicus Data Store. This would allow users to: Track data availability trends Identify recurring download failures Ensure data integrity across updates Share transparent monitoring results with the community","title":"Vision"},{"location":"generated_table/","text":"List of Datasets With Errors dataset_id dataset_version version_part service_name variable_name has_time_coordinate last_available_time region downloadable last_downloadable_time first_command first_error second_command second_error third_command third_error id","title":"Table of errors"},{"location":"generated_table/#list-of-datasets-with-errors","text":"dataset_id dataset_version version_part service_name variable_name has_time_coordinate last_available_time region downloadable last_downloadable_time first_command first_error second_command second_error third_command third_error id","title":"List of Datasets With Errors"},{"location":"usage/","text":"Usage Most of the scripts can be run independently. To run a script: python script_name.py --data-dir path_to_storage Where path_to_storage is the directory where results will be stored. Scripts Overview add_data_in_database Takes the data from a CSV file containing download attempts (downloaded_datasets) and inserts results into a database. check_if_download_errors Searches the downloaded_datasets CSV for failed downloads and returns False if any are found. extracts_datasets_from_describe Creates a CSV containing dataset information using the copernicusmarine.describe command. retrieve_from_describe_test_availabilibity_and_push_in_db Master script. Loads datasets from describe, attempts downloads, stores results in a database, and returns False if downloads fail. test_downloading_datasets Reads list_of_informations_from_the_describe.csv and tries to download all datasets listed. script_to_markdown Creates a markdown page for printing the results of the script treating_outputs Reads the downloaded_datasets CSV and provides basic statistics by region. Execution Order Some scripts depend on others having been run first: retrieve_from_describe_test_availabilibity_and_push_in_db \u2013 standalone extracts_datasets_from_describe \u2013 standalone test_downloading_datasets \u2013 requires step 2 check_if_download_errors \u2013 requires step 3 script_to_markdown - requires step 3 add_data_in_database \u2013 requires step 3 treating_outputs \u2013 requires step 3 Login The script_to_markdown script requires login to github. So far it is down because the login are stored with the command \"git config --global credential.helper store\"","title":"Usage"},{"location":"usage/#usage","text":"Most of the scripts can be run independently. To run a script: python script_name.py --data-dir path_to_storage Where path_to_storage is the directory where results will be stored.","title":"Usage"},{"location":"usage/#scripts-overview","text":"add_data_in_database Takes the data from a CSV file containing download attempts (downloaded_datasets) and inserts results into a database. check_if_download_errors Searches the downloaded_datasets CSV for failed downloads and returns False if any are found. extracts_datasets_from_describe Creates a CSV containing dataset information using the copernicusmarine.describe command. retrieve_from_describe_test_availabilibity_and_push_in_db Master script. Loads datasets from describe, attempts downloads, stores results in a database, and returns False if downloads fail. test_downloading_datasets Reads list_of_informations_from_the_describe.csv and tries to download all datasets listed. script_to_markdown Creates a markdown page for printing the results of the script treating_outputs Reads the downloaded_datasets CSV and provides basic statistics by region.","title":"Scripts Overview"},{"location":"usage/#execution-order","text":"Some scripts depend on others having been run first: retrieve_from_describe_test_availabilibity_and_push_in_db \u2013 standalone extracts_datasets_from_describe \u2013 standalone test_downloading_datasets \u2013 requires step 2 check_if_download_errors \u2013 requires step 3 script_to_markdown - requires step 3 add_data_in_database \u2013 requires step 3 treating_outputs \u2013 requires step 3","title":"Execution Order"},{"location":"usage/#login","text":"The script_to_markdown script requires login to github. So far it is down because the login are stored with the command \"git config --global credential.helper store\"","title":"Login"}]}